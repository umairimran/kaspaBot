The “don’t trust, verify!” slogan is often misunderstood. Many people board airplanes without verifying anything about the pilot or the aircraft; they eat at restaurants without knowing what will be transmitted to their bodies; they take medicines without verifying the supply chain. This raises the question: why would one protect their money with measures they do not take to protect their lives?

This situation reminds me of Jacob, the forefather of Israel, who crossed the Jordan River back to his homeland some 3,500 years ago. He forgot a few small items (think of dust UTXOs), and as later Talmudic Rabbis tell us, he returned to the east bank in the middle of the night to retrieve them. This was apparently unsafe, and he encountered a mysterious figure who wrestled with him, resulting in a limp, a divine blessing, and a new name—Israel. The Talmud concludes: “From here it is derived that the possessions of the righteous are dearer to them than their bodies. And why do they care so much about their possessions? It is because they do not stretch out their hands to partake of stolen property.”

The connection between this story and SPV sync mode based on multiplicative-hash UTXO commitments is clear. Verifying the proper functioning of all external systems one relies on is infeasible, unscalable, and anticivil. Civilization is about scaling up function and trust, and focusing solely on money verification is peculiar and can lead to complications.

Why not fiat? Fiat supply is unpredictably inflated; the political printing of money corrupts the democratic sphere, shifting the political debate from wealth creation to money allocation. Regulators attempt to control financial transfers, eliminate cash, and restrict economic freedom.

Cryptocurrencies offer predictable issuance and censorship resistance. The property that “no fraudulent transaction ever occurred in this currency system” is not the primary concern for users. When joining a cryptocurrency system, checking its issuance and decentralization is far more important than verifying the historical validity of transactions. Users should care about the state of their node being the one most likely to prevail by economic majority, rather than its validity in an abstract sense. The primary goal of consensus systems is to facilitate agreement, not enforce consistency. If the economic majority maintains a ledger that includes an invalid transaction from history, the system can still serve its purpose of facilitating agreement.

Users are protected from unsafe networks as long as they can assume efficient ways to communicate and diffuse information about such issues. This novel method of information diffusion is called the Internet, or its manual predecessor—Civilization. Just as with airplanes and restaurants, users assume they are part of a functioning system, and information about past security breaches would reach them through conventional channels. These channels are typically reliable due to highly-staked, ideological, or philanthropic whistleblowers who continuously verify the ledger, or by previous users harmed by the system’s malfunctions, granting the system herd immunity. Running full nodes is important for the ledger’s society, although the marginal utility is rapidly decreasing.

To summarize: If you are going to join the IOTA network, research it first.

Why not Bitcoin? The Initial Blockchain Download (IBD) is the process by which new nodes join the network. Since Bitcoin core developers adhere to the ethos of “don’t trust, verify!”, the default behavior of a new node is to download and verify the entire history of the Bitcoin ledger. Consequently, Bitcoin’s throughput is deliberately limited to enable fast IBD; processing too many transactions per second today would complicate verification for users joining in the future.

Back in 2013, my advisor sent me a paper titled “Bitcoin: A Peer-to-Peer Electronic Cash System.” The narrative has shifted to “Bitcoin: An Electronic Store of Value,” coupled with the notion that attempts to create a peer-to-peer electronic cash coin are scams.

Why Kaspa? First, to make Satoshi great again. PHANTOM is a neat generalization of Nakamoto Consensus (when k=0, PHANTOM coincides with the longest chain rule). It follows the same principles but supports concurrency. It embodies Satoshi’s vision for electronic cash. We implement PHANTOM because we want to achieve instant transaction confirmations, similar to the results of a Google search or sending an email. This challenge is akin to how Bitcoin core developers chose to work on Taproot—it is innovative and not entirely useless.

Secondly, to make myself rich.

Third, we need a base layer whose trade-offs center around the needs and requests of crypto-informed users. This implies implementing the default node to skip historical verification, making it an optional operation that relies on fewer archival nodes maintained by some entities. This situation mirrors Bitcoin, where most full nodes prune the data necessary for syncing newcomers. However, archival nodes can prove to newcomers that a transaction was fraudulent by providing the Merkle witnesses for inclusion in the blocks involved in the claimed collusion.

Kaspa nodes prune block data by default, and new nodes do not request historical data; instead, they sync in SPV mode by downloading and verifying only block headers. This is not a stronger trust assumption than a history-verifying node; it is simply a different requirement. The node requests the UTXO set from untrusted peers in the network and verifies it against the UTXO commitment embedded inside the latest received header. If there is a mismatch, the node bans the sending peers, requests the UTXO set from new untrusted peers, and repeats the process. If they match, the node verifies that no unexpected inflation occurred by comparing the sum of UTXOs to the specified minting schedule, a comparison for which block headers suffice.

My suggestion to scale up Nakamoto Consensus includes several considerations: 

1. **Latency Constraint on Consensus**: Move from longest-chain to PHANTOM, which is tolerant to and compatible with any predetermined upper bound on latency.
2. **CPU Consumption**: Process fewer transactions per second on L1 while supporting large payloads that are cheap CPU-wise, enabling easy and healthy L2 (e.g., SN/TARK proofs for ZKRUs).
3. **Bandwidth Consumption**: Design sharding of data and data availability proofs, similar to Ethereum 2.0, which is an open research question since PoW has no native identities to serve as the basis for sharding.
4. **Memory Consumption and Disk I/O**: Implement class group-based accumulators that require no trusted setup, allowing for UTXO set pruning and operation as a stateless client. Challenges include the user experience of storing and updating witnesses and weighing memory savings against higher CPU consumption.
5. **Storage**: Prune block data to reduce storage requirements significantly. Consider pruning block headers, which raises questions about how new nodes will be guaranteed to sync to the consensus state and not to a stale or malicious branch. However, any system with deterministic finality relies on weak subjectivity, making the entire history of PoW potentially redundant.
6. **IBD Time**: Implement a DAG-adapted version of FlyClient to reduce the cost of syncing a new node from O(num of blocks in history) to O(log(num of blocks in history)). This does not reduce storage at the syncer but allows the syncee to sync without downloading the entire history of block headers.

In conclusion, Kaspa is PoW on steroids, optimized for informed users rather than ideologues. Its throughput should be constrained by real-time performance considerations, not by the performance of downloading and verifying the historical ledger, which serves as an auxiliary trust gateway rather than the primary pillar of trust in the system.

Kaspa aims to reach 100 blocks per second, and to achieve this, nodes should prune data by default. When new nodes join the network, they should sync against the heaviest PoW DAG, which is the one most likely to represent the current consensus view. Node operators should connect to a sufficient number of peers to avoid being eclipsed from the network, ensuring they are informed about historical mishaps, such as invalid blocks and finality violations, which are recorded and propagated by full nodes to keep new nodes aware of what they are syncing about.